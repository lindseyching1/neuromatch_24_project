{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: all this is right now are a bunch of code snippets that will most likley be valuable to our algo that will need to be modefied "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "#rllib with ray is one possible candidate for the primary framework for the imitation learning \n",
    "from ray import train, tune\n",
    "\n",
    "#RL option after imitation learning is done\n",
    "from agilerl.algorithms.ppo import PPO\n",
    "from agilerl.training.train_on_policy import train_on_policy\n",
    "from agilerl.utils.utils import create_population, make_skill_vect_envs, make_vect_envs\n",
    "from agilerl.wrappers.learning import Skill\n",
    "\n",
    "from ray.rllib.algorithms.bc import BCConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. load rdm and image data: in progress\n",
    "\n",
    "2. create enviroment: in progress\n",
    "\n",
    "3. load the model after imitation learning training: \n",
    "\n",
    "4. \n",
    "\n",
    "\n",
    "\n",
    "observation space: images, actions from other agents\n",
    "\n",
    "action space: expending activity units, sending signal observable by other agent that also expends an activity unit\n",
    "\n",
    "expert data: input: images, output: rdm\n",
    "\n",
    "training stage: use rllib for imitation learning\n",
    "\n",
    "save model in .pt after training\n",
    "\n",
    "prediction stage: use agilerl for prediction\n",
    "\n",
    "\n",
    "enviroment\n",
    "\n",
    "class\n",
    "\n",
    "step()\n",
    "\n",
    "reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class outside(AECEnv):\n",
    "\n",
    "    #goal is for this enviroment to encapsulate everything outside of a given agent\n",
    "\n",
    "    #that means: agents other actions, and stimuli images. \n",
    "\n",
    "    #the step function is where each the actions the other agent has done and after n number a new stimuli and the reward will be updated\n",
    "    #the reset function will simply initialize everything to 0 or some other default value at the beginning of the next step. \n",
    "\n",
    "    def __init__(self, render_mode=None, size=5):\n",
    "        print(\"initializing enviroment\")\n",
    "    def step():\n",
    "        print(\"step\")\n",
    "\n",
    "    def reset():\n",
    "        print(\"new step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BC here stands for behavior cloning \n",
    "\n",
    "config = BCConfig().training(lr=0.00001, gamma=0.99)\n",
    "config = config.offline_data(\n",
    "    input_=\"./rllib/tests/data/cartpole/large.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    config = (\n",
    "        PPOConfig()\n",
    "        .environment(TwoStepGame)\n",
    "        .framework(args.framework)\n",
    "        .env_runners(\n",
    "            batch_mode=\"complete_episodes\",\n",
    "            num_env_runners=0,\n",
    "            # TODO(avnishn) make a new example compatible w connectors.\n",
    "            enable_connectors=False,\n",
    "        )\n",
    "        .callbacks(FillInActions)\n",
    "        .training(model={\"custom_model\": \"cc_model\"})\n",
    "        #.offline_data(input_=\"/tmp/cartpole-out\")\n",
    "        .multi_agent(\n",
    "            policies={\n",
    "                \"pol1\": (None, observer_space, action_space, {}),\n",
    "                \"pol2\": (None, observer_space, action_space, {}),\n",
    "            },\n",
    "            policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"pol1\"\n",
    "            if agent_id == 0\n",
    "            else \"pol2\",\n",
    "            observation_fn=central_critic_observer,\n",
    "        )\n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.marwil import MARWILConfig\n",
    "# Run this from the ray directory root.\n",
    "config = MARWILConfig()  \n",
    "config = config.training(beta=1.0, lr=0.00001, gamma=0.99)  \n",
    "config = config.offline_data(  \n",
    "    input_=[\"./rllib/tests/data/cartpole/large.json\"])\n",
    "print(config.to_dict()) \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Build an Algorithm object from the config and run 1 training iteration.\n",
    "algo = config.build()  \n",
    "algo.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
