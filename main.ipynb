{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "from tqdm import trange\n",
    "\n",
    "import scipy.io as sio\n",
    "\n",
    "#rllib with ray is the primary framework that will be used for imitation learning \n",
    "from ray import air, tune, rllib\n",
    "\n",
    "#RL option after imitation learning is done\n",
    "from agilerl.algorithms.ppo import PPO\n",
    "from agilerl.training.train_on_policy import train_on_policy\n",
    "from agilerl.utils.utils import create_population, make_skill_vect_envs, make_vect_envs\n",
    "from agilerl.wrappers.learning import Skill\n",
    "\n",
    "from ray.rllib.algorithms.bc import BCConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data \n",
    "\n",
    "x = sio.loadmat()\n",
    "y = sio.loadmat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "class CichyEnv(MultiAgentEnv):\n",
    "    def __init__(self, images, expert_rdms):\n",
    "        self.images = images\n",
    "        self.expert_rdms = expert_rdms\n",
    "        self.num_agents = 2\n",
    "        self.agent_ids = [\"IT\", \"EVC\"]\n",
    "\n",
    "\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"image\": spaces.Box(low=0, high=255, shape=(224, 224, 3), dtype=np.uint8),\n",
    "            \"other_action\": spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32)\n",
    "        })\n",
    "\n",
    "        # Continuous: 1. expending activity units 2. sending signal\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.state = {agent: {\"image\": self.images[self.current_step], \"other_action\": np.array([0.0])} for agent in self.agent_ids}\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        obs, rewards, dones, infos = {}, {}, {}, {}\n",
    "        self.current_step += 1\n",
    "\n",
    "        for agent_id in self.agent_ids:\n",
    "            other_agent_id = \"EVC\" if agent_id == \"IT\" else \"IT\"\n",
    "            obs[agent_id] = {\"image\": self.images[self.current_step], \"other_action\": action_dict[other_agent_id][1]}\n",
    "            rewards[agent_id] = self._calculate_reward(agent_id, action_dict[agent_id])\n",
    "            dones[agent_id] = self.current_step >= len(self.images) - 1\n",
    "            infos[agent_id] = {}\n",
    "\n",
    "        dones[\"__all__\"] = all(dones.values())\n",
    "        return obs, rewards, dones, infos\n",
    "\n",
    "    def _calculate_reward(self, agent_id, action):\n",
    "      ## placeholder now,\n",
    "      ## thinking of BIC\n",
    "        return -np.sum((self.expert_rdms[agent_id][self.current_step] - action) ** 2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#image code converts pngs into arrays if they have not already been converted \n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "img = Image.open('lena.png')\n",
    "arr = np.array(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MVP(Minimum viable product) checklist:\n",
    "1. load rdm and image data: in progress\n",
    "\n",
    "2. create imitation learner: in progress\n",
    " - define observation space: in progress\n",
    " - define action space: in progress\n",
    " - definitively settle on imitation learner architecture: DONE \n",
    "\n",
    " RL architecture: MARIWEL\n",
    "\n",
    " it support multiagent RL and support continuous action and observation spaces. \n",
    "\n",
    "3. create enviroment: in progress\n",
    "\n",
    "4. load the model after imitation learning training: \n",
    "note: instead of rllib we can possibly use agilerl instead for everything after step 4\n",
    "5. load the model and proceed with standard RL \n",
    "\n",
    "STREtTCH\n",
    "1. create more agents to carry out simulations on more granular level. \n",
    "\n",
    "MISC\n",
    "\n",
    "observation space: images, actions from other agents\n",
    "\n",
    "action space: expending activity units, sending signal observable by other agent that also expends an activity unit\n",
    "\n",
    "expert data: input: images, output: rdm\n",
    "\n",
    "training stage: use rllib for imitation learning\n",
    "\n",
    "save model in .pt after training\n",
    "\n",
    "prediction stage: use agilerl for prediction\n",
    "\n",
    "why is more RL needed beyond just the imitation learning? because the agents will also be able to interact with each other. This is not captured in the dataset that is being fed to the neural network \n",
    "\n",
    "enviroment\n",
    "\n",
    "class\n",
    "\n",
    "step()\n",
    "\n",
    "reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class raw_env(AECEnv, EzPickle):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"name\": \"connect_four_v3\",\n",
    "        \"is_parallelizable\": False,\n",
    "        \"render_fps\": 2,\n",
    "    }\n",
    "\"\"\"\n",
    "class outside(AECEnv,EzPickle):\n",
    "\n",
    "    #goal is for this enviroment to encapsulate everything outside of a given agent\n",
    "\n",
    "    #that means: agents other actions, and stimuli images. \n",
    "\n",
    "    #the step function is where each the actions the other agent has done and after n number a new stimuli and the reward will be updated\n",
    "    #the reset function will simply initialize everything to 0 or some other default value at the beginning of the next step. \n",
    "\n",
    "    def __init__(self, render_mode=None, size=5):\n",
    "        EzPickle.__init__(\n",
    "            self,\n",
    "            render_mode,\n",
    "\n",
    "            continuous,\n",
    "        )\n",
    "        super().__init__()\n",
    "        print(\"initializing enviroment\")\n",
    "    def step():\n",
    "        print(\"step\")\n",
    "\n",
    "    def reset():\n",
    "        print(\"new step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BC here stands for behavior cloning \n",
    "\n",
    "config = BCConfig().training(lr=0.00001, gamma=0.99)\n",
    "config = config.offline_data(\n",
    "    input_=\"./rllib/tests/data/cartpole/large.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    act_space = Dict(  \n",
    "        {\n",
    "            \"ext_controller\": continuous(1),\n",
    "            \"inner_state\": continuous(1),\n",
    "        })\n",
    "    config = (\n",
    "        PPOConfig()\n",
    "        .environment(CichyEnv)\n",
    "        .framework(args.framework)\n",
    "        .env_runners(\n",
    "            batch_mode=\"complete_episodes\",\n",
    "            num_env_runners=0,\n",
    "            # TODO(avnishn) make a new example compatible w connectors.\n",
    "            enable_connectors=False,\n",
    "        )\n",
    "        .callbacks(FillInActions)\n",
    "        .training(model={\"custom_model\": \"cc_model\"})\n",
    "        #.offline_data(input_=\"/tmp/cartpole-out\")\n",
    "        .multi_agent(\n",
    "            policies={\n",
    "                \"pol1\": (None, observer_space, action_space, {}),\n",
    "                \"pol2\": (None, observer_space, action_space, {}),\n",
    "            },\n",
    "            policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"pol1\"\n",
    "            if agent_id == 0\n",
    "            else \"pol2\",\n",
    "            observation_fn=central_critic_observer,\n",
    "        )\n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    "    ).fit()\n",
    "\n",
    "    if args.as_test:\n",
    "        check_learning_achieved(results, args.stop_reward)\n",
    "\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.marwil import MARWILConfig\n",
    "# Run this from the ray directory root.\n",
    "config = MARWILConfig()  \n",
    "config = config.training(beta=1.0, lr=0.00001, gamma=0.99)  \n",
    "config = config.offline_data(  \n",
    "    input_=[\"./rllib/tests/data/cartpole/large.json\"])\n",
    "config = config.multi_agent(\n",
    "            policies={\n",
    "                \"pol1\": (None, observer_space, action_space, {}),\n",
    "                \"pol2\": (None, observer_space, action_space, {}),\n",
    "            },\n",
    "            policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"pol1\"\n",
    "            if agent_id == 0\n",
    "            else \"pol2\",\n",
    "            observation_fn=central_critic_observer,\n",
    "        )\n",
    "print(config.to_dict()) \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Build an Algorithm object from the config and run 1 training iteration.\n",
    "algo = config.build()  \n",
    "algo.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.marwil import MARWILConfig\n",
    "from ray import tune\n",
    "config = MARWILConfig()\n",
    "# Print out some default values.\n",
    "print(config.beta)  \n",
    "# Update the config object.\n",
    "config.training(lr=tune.grid_search(  \n",
    "    [0.001, 0.0001]), beta=0.75)\n",
    "# Set the config object's data path.\n",
    "# Run this from the ray directory root.\n",
    "config.offline_data( \n",
    "    input_=[\"./rllib/tests/data/cartpole/large.json\"])\n",
    "# Set the config object's env, used for evaluation.\n",
    "config.environment(env=\"CartPole-v1\")  \n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner(  \n",
    "    \"MARWIL\",\n",
    "    run_config=air.RunConfig(stop=stop, verbose=2)\n",
    "    param_space=config.to_dict(),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediciton\n",
    "\n",
    "action = agent.compute_single_action(obs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
