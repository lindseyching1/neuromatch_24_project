{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "from tqdm import trange\n",
    "\n",
    "import scipy.io as sio\n",
    "\n",
    "#rllib with ray is the primary framework that will be used for imitation learning \n",
    "from ray import air, tune, rllib\n",
    "\n",
    "#RL option after imitation learning is done\n",
    "#from agilerl.algorithms.ppo import PPO\n",
    "#from agilerl.training.train_on_policy import train_on_policy\n",
    "#from agilerl.utils.utils import create_population, make_skill_vect_envs, make_vect_envs\n",
    "#from agilerl.wrappers.learning import Skill\n",
    "\n",
    "from ray.rllib.algorithms.bc import BCConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data \n",
    "\n",
    "x = sio.loadmat()\n",
    "y = sio.loadmat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "class CichyEnv(MultiAgentEnv):\n",
    "    def __init__(self, images, expert_rdms):\n",
    "        self.images = images\n",
    "        self.expert_rdms = expert_rdms\n",
    "        self.num_agents = 2\n",
    "        self.agent_ids = [\"IT\", \"EVC\"]\n",
    "\n",
    "\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"image\": spaces.Box(low=0, high=255, shape=(224, 224, 3), dtype=np.uint8),#this shape should actually probably be 175, 175,3\n",
    "            \"other_action\": spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32)\n",
    "        })\n",
    "\n",
    "        # Continuous: 1. expending activity units 2. sending signal\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "\n",
    "        self.action_space = spaces.Dict({ \n",
    "            \"IAU\": spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32),\n",
    "            \"other_action\": spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32)\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.state = {agent: {\"image\": self.images[self.current_step], \"other_action\": np.array([0.0])} for agent in self.agent_ids}\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        obs, rewards, dones, infos = {}, {}, {}, {}\n",
    "        self.current_step += 1\n",
    "\n",
    "        for agent_id in self.agent_ids:\n",
    "            other_agent_id = \"EVC\" if agent_id == \"IT\" else \"IT\"\n",
    "            obs[agent_id] = {\"image\": self.images[self.current_step], \"other_action\": action_dict[other_agent_id][1]}\n",
    "            rewards[agent_id] = self._calculate_reward(agent_id, action_dict[agent_id])\n",
    "            dones[agent_id] = self.current_step >= len(self.images) - 1\n",
    "            infos[agent_id] = {}\n",
    "\n",
    "        dones[\"__all__\"] = all(dones.values())\n",
    "        return obs, rewards, dones, infos\n",
    "    def _calculate_rewards(self):\n",
    "        rewards = {}\n",
    "        \"\"\"\n",
    "        this function iterates over agents and withen each agent iterates over images to create a cosine similarity. it then calculates the reward\n",
    "        for each agent \n",
    "        \n",
    "        this is configured to do one pass where it generates simulated RDM for one subject\n",
    "        \n",
    "        \"\"\"\n",
    "        for agent_id in self.agent_ids:\n",
    "            actions = np.array(self.actions[agent_id])\n",
    "            num_images = len(self.images)\n",
    "            simulated_rdm = np.zeros((num_images, num_images))#\n",
    "\n",
    "            for i in range(num_images):\n",
    "                for j in range(num_images):\n",
    "                    if i != j:\n",
    "                        sim = cosine_similarity(actions[i].reshape(1, -1), actions[j].reshape(1, -1))[0][0]\n",
    "                        simulated_rdm[i, j] = 1 - sim\n",
    "\n",
    "            expert_rdm = self.expert_rdms[agent_id]\n",
    "            rewards[agent_id] = -np.sum((simulated_rdm - expert_rdm) ** 2)\n",
    "        return rewards\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MVP(Minimum viable product) checklist:\n",
    "1. load rdm and image data: in progress\n",
    "\n",
    "2. create imitation learner: in progress\n",
    " - define observation space: in progress\n",
    " - define action space: in progress\n",
    " - definitively settle on imitation learner architecture: DONE \n",
    "\n",
    " RL architecture: MARIWEL\n",
    "\n",
    " it support multiagent RL and support continuous action and observation spaces. \n",
    "\n",
    "3. create enviroment: in progress\n",
    "\n",
    "4. load the model after imitation learning training: \n",
    "note: instead of rllib we can possibly use agilerl instead for everything after step 4\n",
    "5. load the model and proceed with standard RL \n",
    "\n",
    "STREtTCH\n",
    "1. create more agents to carry out simulations on more granular level. \n",
    "\n",
    "MISC\n",
    "\n",
    "observation space: images, actions from other agents\n",
    "\n",
    "action space: expending activity units, sending signal observable by other agent that also expends an activity unit\n",
    "\n",
    "expert data: input: images, output: rdm\n",
    "\n",
    "training stage: use rllib for imitation learning\n",
    "\n",
    "save model in .pt after training\n",
    "\n",
    "prediction stage: use agilerl for prediction\n",
    "\n",
    "why is more RL needed beyond just the imitation learning? because the agents will also be able to interact with each other. This is not captured in the dataset that is being fed to the neural network \n",
    "\n",
    "the RL and Imitation learning can be done in one fell swoop with the MARIWEL architecture since it can become a hybrid RL and imitation learning architecture by varying the beta paraemter \n",
    "\n",
    "enviroment\n",
    "\n",
    "class\n",
    "\n",
    "step()\n",
    "\n",
    "reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BC here stands for behavior cloning \n",
    "\n",
    "config = BCConfig().training(lr=0.00001, gamma=0.99)\n",
    "config = config.offline_data(\n",
    "    input_=\"./rllib/tests/data/cartpole/large.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def central_critic_observer(agent_obs, **kw):\n",
    "    \"\"\"Rewrites the agent obs to include opponent data for training.\"\"\"\n",
    "\n",
    "    new_obs = {\n",
    "        0: {\n",
    "            \"own_obs\": agent_obs[0],\n",
    "            \"opponent_obs\": agent_obs[1],\n",
    "            \"opponent_action\": 0,  # filled in by FillInActions\n",
    "        },\n",
    "        1: {\n",
    "            \"own_obs\": agent_obs[1],\n",
    "            \"opponent_obs\": agent_obs[0],\n",
    "            \"opponent_action\": 0,  # filled in by FillInActions\n",
    "        },\n",
    "    }\n",
    "    return new_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "from ray.rllib.algorithms.marwil import MARWILConfig\n",
    "from ray import tune\n",
    "config = MARWILConfig()\n",
    "# Print out some default values.\n",
    "print(config.beta)  \n",
    "# Update the config object.\n",
    "config.training(lr=tune.grid_search(  \n",
    "    [0.001, 0.0001]), beta=0.75,gamma=0.99)\n",
    "# Set the config object's data path.\n",
    "# Run this from the ray directory root.\n",
    "config.offline_data( \n",
    "    input_=[\"./rllib/tests/data/cartpole/large.json\"])\n",
    "config = config.multi_agent(\n",
    "    policies={\n",
    "            \"pol1\": (None, observer_space, action_space, {}),\n",
    "            \"pol2\": (None, observer_space, action_space, {}),\n",
    "        },\n",
    "        policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"pol1\"\n",
    "        if agent_id == 0\n",
    "        else \"pol2\",\n",
    "        observation_fn=central_critic_observer,#lambda agent_id, episode, worker, **kwargs: \"pol2\"\n",
    "    )\n",
    "# Set the config object's env, used for evaluation.\n",
    "config.environment(env=CichyEnv())  \n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "\n",
    "\"\"\"\n",
    "pbt_scheduler = PopulationBasedTraining(\n",
    "    time_attr='training_iteration',\n",
    "    metric='episode_reward_mean',#'loss',\n",
    "    mode='min',\n",
    "    perturbation_interval=1,\n",
    "    hyperparam_mutations={\n",
    "        #\"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "        \"alpha\": tune.uniform(0.0, 1.0),\n",
    "    }\n",
    ")\n",
    "pb2_scheduler = PB2(\n",
    "    time_attr='training_iteration',#'time_total_s',\n",
    "    metric='episode_reward_mean',#'mean_accuracy',\n",
    "    mode='min',\n",
    "    perturbation_interval=600.0,\n",
    "    hyperparam_bounds={\n",
    "        #\"lr\": [1e-3, 1e-5],\n",
    "        \"alpha\": [0.0, 1.0],\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "tune.Tuner(  \n",
    "    \"MARWIL\",\n",
    "    run_config=air.RunConfig(stop=stop, verbose=2),\n",
    "    param_space=config.to_dict(),\n",
    "    \"\"\"\n",
    "    tune_config=tune.TuneConfig(\n",
    "        num_samples=2,#4,\n",
    "        scheduler=pb2_scheduler,\n",
    "        #reuse_actors=True,\n",
    "        ),\n",
    "    \"\"\"\n",
    ").fit()\n",
    "\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from ray.rllib.algorithms.marwil import MARWILConfig\n",
    "from ray import tune\n",
    "config = MARWILConfig()\n",
    "# Print out some default values.\n",
    "print(config.beta)  \n",
    "# Update the config object.\n",
    "config = config.training(beta=1.0, lr=0.00001, gamma=0.99) \n",
    "# Set the config object's data path.\n",
    "# Run this from the ray directory root.\n",
    "config.offline_data( \n",
    "    input_=[\"./rllib/tests/data/cartpole/large.json\"])\n",
    "config = config.multi_agent(\n",
    "    policies={\n",
    "            \"pol1\": (None, observer_space, action_space, {}),\n",
    "            \"pol2\": (None, observer_space, action_space, {}),\n",
    "        },\n",
    "        policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"pol1\"\n",
    "        if agent_id == 0\n",
    "        else \"pol2\",\n",
    "        observation_fn=central_critic_observer,#lambda agent_id, episode, worker, **kwargs: \"pol2\"\n",
    "    )\n",
    "# Set the config object's env, used for evaluation.\n",
    "config.environment(env=CichyEnv())  \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Build an Algorithm object from the config and run 1 training iteration.\n",
    "algo = config.build()  \n",
    "algo.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediciton\n",
    "\n",
    "action = agent.compute_single_action(obs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
